{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Use h1 for printing on A3 page -->\n",
    "\n",
    "<h1 style='float:left; font-variant: small-caps;' align='left'>Morphological Priori Constrained Object Detection of Key Structures in Infant Fundus Image</h1>\n",
    "<!--h2 style='float:left; font-variant: small-caps;' align='center'>Morphological Priori Constrained Object Detection of Key Structures in Infant Fundus Image</h2-->\n",
    "\n",
    "\n",
    "<br/><br/><br/>\n",
    "\n",
    "Cite our paper if you use the odn package or the dataset:  \n",
    "> \"Morphological Rule-Constrained Object Detection of Key Structures in Infant Fundus Image\", IEEE/ACM Transactions on Computational Biology and Bioinformatics, 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../notebooks/fundus.png' width='50%'>\n",
    "\n",
    "The scheme of fundus. OD = right eye. OS = left eye. The zones and clock hours are used to localize the distribution of pathological structurers. The distribution is very important for assessing the disease severity. Adapted from the International Committee for the Classification of Retinopathy of Prematurity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zone 1 – Circle from centre of disc with radius of twice distance from disc to macula   \n",
    "Zone 2 – From nasal edge of zone 1 to ora nasally and upto equatot region of retina temporally   \n",
    "Zone 3 – From temporal crescent of retina anterior to zone II  \n",
    "optic disc -  3 to 4 mm to the nasal side of the fovea. It is a vertical oval, with average dimensions of 1.76mm horizontally by 1.92mm vertically. There is a central depression, of variable size, called the optic cup.  \n",
    "macula - has a diameter of around 5.5 mm (0.22 in). The fovea is located near the center of the macula. It is a small pit that contains the largest concentration of cone cells.\n",
    "\n",
    "\n",
    "## Devices we use:   \n",
    "\n",
    "RetCam Wide-Field Digital Imaging System from Clarity Medical Systems (<a title=\"July 06, 2016 (GLOBE NEWSWIRE) -- Natus Medical Incorporated today announced that it has acquired the portfolio of RetCam imaging systems from Clarity Medical Systems, Inc. for $10.6 million.\">now owned by Natus Medical</a>).   \n",
    "PanoCam™ LT Wide-field Digital Imaging System by Visunex Medical Systems  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Setup (Ubuntu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'apt-get' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'apt-get' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!apt-get update\n",
    "!apt-get install ffmpeg libsm6 libxext6  -y\n",
    "!pip install tensorflow odn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subject Demographics, Such As Gender, Birth Weight, Gestational Age, etc.\n",
    "\n",
    "The data are calculated by SZEH's EMRS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from odn.fundus import demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographics.piechart_preterm([62.1,37.9]) # statistics from SEH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographics.piechart_gender([56.5, 43.5]) # statistics from SEH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "demographics.piechart_gender([10.9, 89.1]) # statistics from SEH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Load data ###\n",
    "import numpy as np\n",
    "data = np.genfromtxt ('../data/fundus/GestationalAgeWeek_null_purged.csv', delimiter=\",\") # week + day\n",
    "\n",
    "gw = []\n",
    "for row in data:\n",
    "    if (row[1] and row[1] != np.nan):\n",
    "        gw.append(float(row[0]) + float(row[1])/7.0)\n",
    "    else:\n",
    "        gw.append(float(row[0]))\n",
    "\n",
    "### Histogram ###\n",
    "demographics.hist_gw(gw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data = np.genfromtxt ('../data/fundus/BirthWeight_null_purged.csv', delimiter=\",\") # week + day\n",
    "        \n",
    "demographics.hist_bw(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Object Detection - FRCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Annotation\n",
    "\n",
    "A data annotation tool is developed based on the \"VGG Image Annotator (VIA)\" tool from Oxford University.  \n",
    "\n",
    "[Download link](https://ieee-dataport.s3.amazonaws.com/open/15419/annotate_tool.zip?response-content-disposition=attachment%3B%20filename%3D%22annotate_tool.zip%22&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAJOHYI4KJCE6Q7MIQ%2F20191009%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20191009T194155Z&X-Amz-SignedHeaders=Host&X-Amz-Expires=86400&X-Amz-Signature=99d664cc9b92ed854daf1625208a62fa0484602ff21cb21ccf3933b55dfb9218)\n",
    "\n",
    "\n",
    "The original output format of VIA is JSON. Convert JSON to CSV.\n",
    "> target CSV file format:  \n",
    "> filename,width,height,class,xmin,ymin,xmax,ymax   \n",
    "> file-146.jpg,275,183,object1,4,4,271,180"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View the ground truth images\n",
    "\n",
    "One image files may have multiple ROIs. Get unique image files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from odn.fundus import dataset\n",
    "%matplotlib inline\n",
    "\n",
    "dataset.synthesize_anno(label_file = '../data/fundus/all_labels.csv', \n",
    "    dir_images = '../data/fundus/images/', \n",
    "    dir_output = '../data/fundus/ground_truth/',    \n",
    "    verbose = True,\n",
    "    display = 5 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/><br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n",
    "\n",
    "For details on how to train the models, refer to the [README.md](https://github.com/zhangys11/odn) of odn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example training curves from Y2021\n",
    "\n",
    "<img src ='../notebooks/training_curves.png' width='70%'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Testing\n",
    "\n",
    "```\n",
    "python test_frcnn.py --network vgg16 -p ../../data/fundus/test_public.txt --load models/vgg16/19e.hdf5 --num_rois 32 --write \n",
    "```\n",
    "--num_rois: Number of ROIs per iteration. Higher means more memory use. Works like batch_size\n",
    "--write: output annotated images to outputs\n",
    "\n",
    "\n",
    "A `candidate_rois.txt` with all candidate ROIs is generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_set = pd.read_csv('../data/fundus/test_public.txt', header=None)\n",
    "\n",
    "print('headers: xmin, ymin, xmax, ymax')\n",
    "test_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Morphological Priori Rules\n",
    "\n",
    "<img src='../notebooks/fundus_sample.jpg' width = '40%'>\n",
    "\n",
    "The optic disc is placed 3 to 4 mm to the nasal side of the fovea. It is a vertical oval, with average dimensions of 1.76mm horizontally by 1.92mm vertically. There is a central depression, of variable size, called the optic cup.\n",
    "\n",
    "The macula in humans has a diameter of around 5.5 mm (0.22 in). The fovea is located near the center of the macula. It is a small pit that contains the largest concentration of cone cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A priori knowledge, in Western philosophy since the time of Immanuel Kant, knowledge that is independent of all particular experiences, as opposed to a posteriori knowledge, which derives from experience.\n",
    "\n",
    "ultra-widefi eld imaging\n",
    "\n",
    "Priori 1: Maximum number of optic disc and macula is 1.  \n",
    "Priori 2: On a 1000-pixel-width image, macula diameter is 160 (16%), optic disc diameter is 100 (10%). Determined by the fixed FOV. No matter the fundus camera types or resolutions   \n",
    "Priori 3: On a 1000-pixel-width image, distance between the centers of optic disc and macula (fovea) is 320 (32%). Rule out very near or far macula candidates.   \n",
    "Priori 4: On a standard fundus image, the optic disc and macula are almost positioned in a same horizontal line.      \n",
    "Priori 5: For right eye (OD) image, macula is located on the left side of optic disc; for left eye image, macula is on the right of optic disc.\n",
    "\n",
    "Detection of optic disc is much easier than macula. Use the dectected optic disc as an anchor or reference landmark. Then use priori 2 & 3 & 4 & 5 to rule out excessive maculas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter Candidate ROIs by Priori Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from odn.fundus import annotation\n",
    "\n",
    "annotation.rule_filter_rois(input_file = '../src/odn/candidate_rois.txt', \n",
    "    output_file = '../src/odn/rois.txt',\n",
    "    verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The filtered ROIs are stored in rois.txt file**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive ROI Filtering, i.e. Keep the ROI with largest probability\n",
    "\n",
    "For comparison purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation.naive_filter_rois(input_file = '../src/odn/candidate_rois.txt', \n",
    "    output_file = '../src/odn/rois_naive.txt',\n",
    "    verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Render Images with ROIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw ROIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from odn.fundus import dataset\n",
    "\n",
    "dataset.synthesize_anno('../src/odn/candidate_rois.txt', \n",
    "    dir_images = '../data/fundus/images/', \n",
    "    dir_output = '../data/fundus/frcnn_19e_raw/',    \n",
    "    verbose = True,\n",
    "    display = 5 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtered ROIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.synthesize_anno('../src/odn/rois.txt', \n",
    "    dir_images = '../data/fundus/images_public/', \n",
    "    dir_output = '../data/fundus/frcnn_19e/',    \n",
    "    verbose = True,\n",
    "    display = 5 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from odn.fundus import dataset\n",
    "%matplotlib inline\n",
    "\n",
    "dataset.synthesize_anno('../src/odn/rois.txt', \n",
    "    dir_images = '../data/fundus/images/', \n",
    "    dir_output = '../data/fundus/frcnn_19e_zones/',    \n",
    "    verbose = True, drawzones = True,\n",
    "    display = 5 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive ROIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.synthesize_anno('../src/odn/rois_naive.txt', \n",
    "    dir_images = '../data/fundus/images/', \n",
    "    dir_output = '../data/fundus/frcnn_19e_naive/',    \n",
    "    verbose = True,\n",
    "    display = 5 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation by Numerical Metrics \n",
    "\n",
    "Metric 1 - IOU   \n",
    "Metric 2 - RCE (Relative Center Error)\n",
    "\n",
    "## Calculate performance metrics for our method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from odn import metrics\n",
    "\n",
    "IOU_O , IOU_M ,\tRCE_O, RCE_M , P_O, P_M = metrics.fundus_metrics(gt = '../data/fundus/all_labels.csv', pred = '../src/odn/rois.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate performance metrics for naive filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IOU_O_NAIVE, IOU_M_NAIVE, RCE_O_NAIVE, RCE_M_NAIVE, P_O_NAIVE, P_M_NAIVE = metrics.fundus_metrics(gt = '../data/fundus/all_labels.csv', pred = '../src/odn/rois_naive.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the Result into One Image\n",
    "\n",
    "Display the ground truth, raw ROIs from base-ODN, and final ROIs side by side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metrics.fundus_compare_metrics(gt = '../data/fundus/all_labels.csv', \n",
    "pred = '../src/odn/rois.txt', \n",
    "output_file = './comparison_with_metrics.jpg',\n",
    "image_dirs = [\n",
    "\t'../data/fundus/ground_truth_public', \n",
    "\t'../data/fundus/frcnn_19e_raw',   \n",
    "\t'../data/fundus/frcnn_19e_naive',\n",
    "\t'../data/fundus/frcnn_19e'], verbose = True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../notebooks/compare_chosen -rev.jpg' width='70%'>\n",
    "\n",
    "Some samples from the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Histograms of Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.metric_histogram(gt = '../data/fundus/all_labels.csv', \n",
    "pred = '../src/odn/rois.txt',\n",
    "output_file = './metrics.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HTML style table output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.fundus_compare_metrics_html(gt = '../data/fundus/all_labels.csv', \n",
    "pred = '../src/odn/rois.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.fundus_compare_metrics_html(gt = '../data/fundus/all_labels.csv', \n",
    "pred = '../src/odn/rois_naive.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show images with different results between naive method and our method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image_subset = metrics.fundus_compare_metrics_html2(gt = '../data/fundus/all_labels.csv', \n",
    "pred1 = '../src/odn/rois.txt', pred2 = '../src/odn/rois_naive.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From left to right: ground truth | candidate ROIs from base-ODN | Naive Filtering | Our Method\n",
    "\n",
    "<img src='../notebooks/comparison_with_metrics_DIFF.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/><hr/><br/><br/>\n",
    "\n",
    "# 2. Object Detection - TF-SSD\n",
    "\n",
    "TF-SSD is the second model we explored after FR-CNN."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from odn.fundus import annotation\n",
    "from odn import utils\n",
    "\n",
    "FILES = utils.get_all_images_in_dir(folder = 'YOUR_FUNDUS_IMAGE_FOLDER_PATH_TO_PREDICT')\n",
    "\n",
    "detection_graph, category_index = annotation.load_tf_graph(ckpt_path = '../src/odn/tf_ssd/export/frozen_inference_graph.pb',\n",
    "                 label_path = '../src/odn/tf_ssd/fundus_label_map.pbtxt', \n",
    "                 num_classes = 2)\n",
    "\n",
    "annotation.tf_batch_object_detection(detection_graph, category_index, FILES, \n",
    "                                  'inplace', \n",
    "                                  '../data/fundus/ssd_20220610.txt',                        \n",
    "                                  new_img_width = 900, fontsize = None, suffix = '_SSD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/><hr/><br/><br/>\n",
    "\n",
    "# 3. Object Detection - YOLO5\n",
    "\n",
    "YOLO5 is the lastest model we explored. It also support one-exam batch object detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from odn.fundus import annotation\n",
    "\n",
    "annotation.torch_batch_object_detection_for_one_exam(\n",
    "    model_path = '../src/odn/torch_yolo/runs/train/exp15/weights/best.pt',\n",
    "    input_path = '../data/fundus/one_exam', # or a 'filelist.txt' file\n",
    "    conf_thres=0.3, iou_thres=0.5, max_det=2, \n",
    "    anno_pil = True, colors = [(200,100,100),(55,125,125)],\n",
    "    suffix = '_YOLO5', display = True, verbose = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
